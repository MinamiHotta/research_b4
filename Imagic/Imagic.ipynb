{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 15:42:24 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 546.09                 Driver Version: 546.09       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A    0C    P3               9W /  30W |      0MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ldm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# インストールできていなさそう\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDIMSampler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m instantiate_from_config\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ldm'"
     ]
    }
   ],
   "source": [
    "# インストールできていなさそう\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorchを使用して画像生成モデルをロード\n",
    "from pyexpat import model\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, device=\"cpu\", verbose=False):\n",
    "# MEMO: \"config\" とはモデルの設定を記したファイルのパス\n",
    "# MEMO: \"ckpt\" とは学習済みモデルのチェックポイント(モデルの重みや状態を保存)へのパス\n",
    "# MEMO: \"pl_sd\" とは\"PyTorch Lightning State Dictionary\"の略\n",
    "    if isinstance(config, (str, Path)): # configがパスの場合\n",
    "        config = OmegaConf.load(config) \n",
    "        \n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\") \n",
    "    global_step = pl_sd[\"global_step\"]\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.to(device) # モデルをCPUに配置\n",
    "    model.eval() # モデルを推論モード(evaluation)に\n",
    "    model.cond_stage_model.device = device\n",
    "    return model\n",
    "\n",
    "# モデルを使用し画像を生成 \n",
    "@torch.no_grad()       \n",
    "def sample_model(model, sampler, c, h, w, ddim_steps, scale, ddim_eta, start_code=None, n_samples=1):\n",
    "# MEMO: \"ddim_steps\" とは，ノイズを付与した回数（つまりT）\n",
    "# MEMO: \"ddim_eta\" とは，[0,1]の範囲でノイズの強度を表すパラメータ\n",
    "    uc = None\n",
    "    if scale != 1.0: # 必要に応じて学習条件を取得\n",
    "        uc = model.get_learned_conditioning(n_samples * [\"\"])\n",
    "    \n",
    "    shape = [4, h // 8, w // 8]\n",
    "    # 画像を生成\n",
    "    samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                    conditioning=c,\n",
    "                                    batch_size=n_samples,\n",
    "                                    shape=shape,\n",
    "                                    verbose=False,\n",
    "                                    start_code=start_code,\n",
    "                                    unconditional_guidance_scale=scale,\n",
    "                                    unconditional_conditioning=uc,\n",
    "                                    eta=ddim_eta)\n",
    "    return samples_ddim\n",
    "\n",
    "# 画像の前処理\n",
    "def load_img(path, target_size=512):\n",
    "    image = Image.open(path).convert(\"RGB\") # PIL形式の画像をRGBに変換\n",
    "    \n",
    "    # 画像をテンソルに変換\n",
    "    t_form = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image = t_form(image)\n",
    "    \n",
    "    # [-1,1]の範囲に正規化して変換\n",
    "    return 2. * image -1.\n",
    "\n",
    "# 生成された画像をデコードしPILに変換\n",
    "def decode_to_im(samples, n_samples=1, nrow=1):\n",
    "    samples = model.decode_first_stage(samples)\n",
    "    ims = torch.clamp((samples + 1.0) / 2.0, min=0.0, max=1.0) #[0,1]の範囲に変換\n",
    "    x_sample = 255 * rearrange(ims.cpu().numpy(), '(n1 n2) c h w -> (n1 h) (n2 w) c', n1=n_samples//nrow, n2=nrow) #[0,255]の範囲に変換\n",
    "    \n",
    "    return Image.fromarray(x_sample.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = \"cuda:0\" # CUDAを使用しGPU(IDが0)上で計算を実行\n",
    "config = \"configs/stable-diffusion/v1-inference.yaml\"\n",
    "ckpt = \"models/ldm/stable-diffusion-v1/sd-v1-4-full-ema.ckpt\"\n",
    "input_img = \"\" #TODO: 自分で設定\n",
    "prompt = \"A photo of \" #TODO: 自分で設定\n",
    "\n",
    "# パラメータ\n",
    "scale = 3\n",
    "h = 512\n",
    "w = 512\n",
    "ddim_steps = 45\n",
    "ddim_eta = 0.0\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = load_model_from_config(config, ckpt, device)\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "init_image = load_img(input_img).to(device).unsqueeze(0) # 入力画像を読み込み，デバイスに配置\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image)) # 初期画像から初期潜在変数を取得(e_opt)\n",
    "decode_to_im(init_latent) # 初期潜在変数から画像を生成し表示(e_opt→画像)\n",
    "\n",
    "orig_emb = model.get_learned_conditioning([prompt]) # テキストプロンプトから潜在変数を取得(e_tgt)\n",
    "emb = orig_emb.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_sample = lambda x, s, code: decode_to_im(sample_model(model, sampler, x, h, w, ddim_steps, s, ddim_eta, start_code=code))\n",
    "start_code = torch.randn_like(init_latent)\n",
    "quick_sample(emb, scale, start_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the embedding\n",
    "emb.requires_grad = True # 埋め込みベクトルの勾配を有効化, e_tgt→e_optを探索\n",
    "lr = 0.001 # 学習率\n",
    "it = 500 # 学習のイテレーション数，繰り返し\n",
    "opt = torch.optim.Adam([emb], lr=lr) # e_tgt→e_optの最適化, 最適化にAdamを使用\n",
    "criteria = torch.nn.MSELoss() # 平均二乗誤差損失\n",
    "history = []\n",
    "\n",
    "pbar = tqdm(range(it)) # 学習の進捗バー\n",
    "for i in pbar:\n",
    "    opt.zero_grad() # 初期勾配は0\n",
    "    \n",
    "    noise = torch.randn_like(init_latent) # ノイズを生成\n",
    "    t_enc = torch.randint(1000, (1,), device=device) # これが徐々に更新されてe_optに\n",
    "    z = model.q_sample(init_latent, t_enc, noise=noise)\n",
    "    \n",
    "    pred_noise = model.apply_model(z, t_enc, emb) # 逆拡散過程でノイズを予測\n",
    "    \n",
    "    loss = criteria(pred_noise, noise)\n",
    "    loss.backward() # 損失に対する勾配計算\n",
    "    pbar.set_postfix({\"loss\": loss.item()})\n",
    "    history.append(loss.item()) # パラメータ更新\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history)\n",
    "plt.show()\n",
    "quick_sample(emb, scale, start_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "emb.requires_grad = False # 埋め込みベクトルの勾配を無効化, e_optはそのままに，より適合するようにDiffusion Modelを学習させる\n",
    "model.train()\n",
    "\n",
    "lr = 1e-6\n",
    "it = 1000\n",
    "opt = torch.optim.Adam(model.model.parameters(), lr=lr)\n",
    "criteria = torch.nn.MSELoss()\n",
    "history = []\n",
    "\n",
    "pbar = tqdm(range(it))\n",
    "for i in pbar:\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    noise = torch.randn_like(init_latent)\n",
    "    t_enc = torch.randint(model.num_timesteps, (1,), device=device)\n",
    "    z = model.q_sample(init_latent, t_enc, noise=noise)\n",
    "    \n",
    "    pred_noise = model.apply_model(z, t_enc, emb)\n",
    "    \n",
    "    loss = criteria(pred_noise, noise)\n",
    "    loss.backwards()\n",
    "    pbar.set_postfix({\"loss\": loss.item()})\n",
    "    history.append(loss.item())\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "quick_sample(emb, scale, start_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "etas = [0.0, 0.25, 0.50, 0.75, 1.0] # TODO: ηの値を設定\n",
    "for eta in etas:\n",
    "    new_emb = eta * orig_emb + (1 - eta) * emb\n",
    "    display(quick_sample(new_emb, scale, start_code))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
